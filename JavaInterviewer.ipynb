{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88cfc193",
   "metadata": {},
   "source": [
    "下载对应版本的transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb8628",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.41.2\n",
    "!pip install -q accelerate==0.30.1\n",
    "!pip install -q peft==0.11.1\n",
    "!pip install -q trl==0.8.6\n",
    "!pip install -q datasets==2.19.0\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q fsspec==2025.3.0\n",
    "!pip install -q gcsfs==2025.3.0\n",
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af5973",
   "metadata": {},
   "source": [
    "由于要读取json文件的内容，因此需要在colab上将drive挂载到该python notebook中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d099e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a820a",
   "metadata": {},
   "source": [
    "查看一下GPU信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a608341",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21cceb",
   "metadata": {},
   "source": [
    "导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from openai import OpenAI\n",
    "# import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bba71",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/content/drive/MyDrive/java_sft/data/java_interview.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "# 默认会有一个 \"train\" split，这里再划分出验证集\n",
    "dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612afbbb",
   "metadata": {},
   "source": [
    "加载模型（Qwen/Qwen2.5-3B-Instruct）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5113f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"  # 比原来的 Base 版更适合做 Chat 微调\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 有些 Qwen 没显式 pad_token，这里兜底一下\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    load_in_4bit=True,     # 4bit 量化\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7a899",
   "metadata": {},
   "source": [
    "构建 LoRA 微调配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55684584",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    target_modules=[\"qkv_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef1281",
   "metadata": {},
   "source": [
    "构造 SFTTrainer（核心训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ef13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/java_sft/qwen-java-sft\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=200,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "# def format_example(example):\n",
    "#     prompt = (\n",
    "#         f\"<|im_start|>system\\n{example['system']}<|im_end|>\\n\"\n",
    "#         f\"<|im_start|>user\\n{example['input']}<|im_end|>\\n\"\n",
    "#         f\"<|im_start|>assistant\\n{example['output']}<|im_end|>\"\n",
    "#     )\n",
    "#     return [prompt]\n",
    "\n",
    "def format_example(batch):\n",
    "    results = []\n",
    "\n",
    "    # batch[\"system\"] 是一个 list，例如 [\"你是面试官\", \"你是面试官\", ...]\n",
    "    for sys_msg, usr_msg, asst_msg in zip(batch[\"system\"], batch[\"input\"], batch[\"output\"]):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": usr_msg},\n",
    "            {\"role\": \"assistant\", \"content\": asst_msg},\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        results.append(text)\n",
    "\n",
    "    return results    # <-- 必须是 list[str]\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=None,\n",
    "    max_seq_length=1024,\n",
    "    formatting_func=format_example,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210365c",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344a34b",
   "metadata": {},
   "source": [
    "保存 LoRA 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83754198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里保存的是 LoRA + 基座的 PeftModel\n",
    "trainer.model.save_pretrained(\"/content/drive/MyDrive/java_sft/qwen-java-sft\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/java_sft/qwen-java-sft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd70dfd",
   "metadata": {},
   "source": [
    "推理测试（加载 LoRA 模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944752bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. 加载微调前模型（Base Model）\n",
    "# ---------------------------\n",
    "base_model_name = model_name  # 和上面保持一致\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 加载微调后模型（LoRA + SFT）\n",
    "# ---------------------------\n",
    "ft_model_path = \"/content/drive/MyDrive/java_sft/qwen-java-sft\"\n",
    "\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    ft_model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if ft_tokenizer.pad_token is None:\n",
    "    ft_tokenizer.pad_token = ft_tokenizer.eos_token\n",
    "\n",
    "# 关键：使用 AutoPeftModelForCausalLM 加载带 LoRA 的模型\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    ft_model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=base_tokenizer)\n",
    "# ft_pipe = pipeline(\"text-generation\", model=ft_model, tokenizer=ft_tokenizer)\n",
    "ft_model = ft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a506939",
   "metadata": {},
   "source": [
    "定义答案生成与答案对比函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(pipe, tokenizer, question, max_new_tokens=256):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一名 Java 面试官。\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 先拿到 prompt 的长度\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(pipe.model.device)\n",
    "    # prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # outputs = pipe(\n",
    "    #     prompt,\n",
    "    #     max_new_tokens=max_new_tokens,\n",
    "    #     do_sample=False,   # 面试场景更追求稳定，可以先关掉 sampling\n",
    "    #     pad_token_id=tokenizer.eos_token_id\n",
    "    # )[0][\"generated_text\"]\n",
    "\n",
    "    # # 重新用 tokenizer 编码解码，截取「新生成的」部分\n",
    "    # output_ids = tokenizer(outputs, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    # new_tokens = output_ids[prompt_len:]  # 只取新生成 token\n",
    "\n",
    "    # answer = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    # return answer\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,    # 不采样 → 稳定可复现\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    new_tokens = outputs[0][prompt_len:]\n",
    "    answer = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return answer\n",
    "\n",
    "# def compare_answer(question, max_new_tokens=256):\n",
    "#     base_answer = generate_answer(base_pipe, base_tokenizer, question, max_new_tokens)\n",
    "#     ft_answer = generate_answer(ft_pipe, ft_tokenizer, question, max_new_tokens)\n",
    "\n",
    "#     # print(\"========【微调前】========\")\n",
    "#     # print(base_answer)\n",
    "#     # print(\"\\n========【微调后】========\")\n",
    "#     # print(ft_answer)\n",
    "\n",
    "#     return base_answer, ft_answer\n",
    "def compare_answer(question, max_new_tokens=256):\n",
    "    base_answer = generate_answer(base_model, base_tokenizer, question, max_new_tokens)\n",
    "    ft_answer = generate_answer(ft_model, ft_tokenizer, question, max_new_tokens)\n",
    "\n",
    "    return base_answer, ft_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dfbb03",
   "metadata": {},
   "source": [
    "验证及对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ef654",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Redis的持久化策略有两种，分别是什么？以及两者的区别是什么？\"\n",
    "res = compare_answer(question, 1024)\n",
    "print(\"========【微调前】========\")\n",
    "print(res[0])\n",
    "print(\"\\n========【微调后】========\")\n",
    "print(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7397ff",
   "metadata": {},
   "source": [
    "批量验证结果并输出为JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 设置 DeepSeek API KEY\n",
    "# ======================\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-739e4ade526e4316afe7789f040e93bf\",\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 评分 Rubric\n",
    "# ======================\n",
    "RUBRIC = \"\"\"\n",
    "你是一名专业的 Java 技术面试官与评分裁判。\n",
    "请严格依据以下评分标准对“模型回答”进行客观、公正的评分，总分 100 分。\n",
    "\n",
    "=====================\n",
    "【评分维度与细则（详细说明）】\n",
    "=====================\n",
    "\n",
    "1. 技术正确性（0–40 分）\n",
    "- 回答是否准确描述了概念、原理、机制或流程？\n",
    "- 是否存在技术性错误、误解或不严谨之处？\n",
    "- 是否能覆盖问题核心点？\n",
    "- 若存在关键性错误，分数大幅降低。\n",
    "\n",
    "评分区间示例：\n",
    "- 35–40：完全正确、内容严谨、无错误，技术细节准确。\n",
    "- 25–34：整体正确，但存在轻微遗漏或浅层不严谨。\n",
    "- 10–24：部分正确，但有明显理解偏差。\n",
    "- 0–9：技术上错误严重、明显误解问题。\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "2. 简洁性（0–20 分）\n",
    "- 是否避免冗余或长篇大论？\n",
    "- 是否以面试官喜欢的方式快速给出重点？\n",
    "\n",
    "评分区间示例：\n",
    "- 18–20：非常简洁，关键点一句话即可理解。\n",
    "- 13–17：基本简洁，但稍有赘述。\n",
    "- 6–12：存在较多废话或不必要的解释。\n",
    "- 0–5：内容冗长、偏科普文章风格。\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "3. 结构化程度（0–15 分）\n",
    "- 回答是否具有清晰结构（如“概念 → 原理 → 优缺点 → 场景”）？\n",
    "- 是否使用了条列式、分点式、层次结构？\n",
    "\n",
    "评分区间示例：\n",
    "- 13–15：结构优秀，逻辑强，可直接作为面试标准答案。\n",
    "- 9–12：有一定结构，但不够突出。\n",
    "- 4–8：结构松散但可理解。\n",
    "- 0–3：缺乏结构，思路混乱。\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "4. 面试场景适配度（0–15 分）\n",
    "- 回答是否像面试者应答？而不是像博客、百科？\n",
    "- 是否避免过度展开？\n",
    "- 是否符合真实 Java 后端面试环境？\n",
    "\n",
    "评分区间示例：\n",
    "- 13–15：完全符合面试风格，重点明确、直击痛点。\n",
    "- 9–12：大体符合，但稍微啰嗦或偏文档风格。\n",
    "- 4–8：偏科普写法，不像面试交流。\n",
    "- 0–3：完全不符合面试语境。\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "5. 专业深度（0–10 分）\n",
    "- 是否体现出对技术的理解深度？\n",
    "- 是否涉及底层细节、性能影响、设计动机、使用场景？\n",
    "\n",
    "评分区间示例：\n",
    "- 9–10：展现深入洞察，体现资深工程师水平。\n",
    "- 6–8：中等深度，覆盖表层 + 中层内容。\n",
    "- 3–5：仅停留在表层定义。\n",
    "- 0–2：内容非常浅显，没有专业性。\n",
    "\n",
    "=====================\n",
    "【输出要求】\n",
    "请只输出 JSON，格式如下：\n",
    "\n",
    "{\n",
    "  \"technical_correctness\": x,\n",
    "  \"conciseness\": x,\n",
    "  \"structure\": x,\n",
    "  \"interview_appropriateness\": x,\n",
    "  \"professional_depth\": x,\n",
    "  \"overall\": x\n",
    "}\n",
    "\n",
    "=====================\n",
    "请严格遵守以上规则进行评分，不要加入额外解释。\n",
    "\"\"\"\n",
    "\n",
    "# ======================\n",
    "# 评分函数\n",
    "# ======================\n",
    "def score_answer(question, answer):\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "【问题】\n",
    "{question}\n",
    "\n",
    "【回答】\n",
    "{answer}\n",
    "\n",
    "请依据评分细则严格评分，并输出必须JSON。\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": RUBRIC},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except:\n",
    "        print(\"JSON 解析失败：\", content)\n",
    "        return {}\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 读取问题并评估\n",
    "# ======================\n",
    "input_file = \"/content/drive/MyDrive/java_sft/data/test_questions_shuffled_50.txt\"\n",
    "output_jsonl = \"/content/drive/MyDrive/java_sft/data/evaluation_results_deepseek.jsonl\"\n",
    "output_csv = \"/content/drive/MyDrive/java_sft/data/evaluation_results_deepseek.csv\"\n",
    "\n",
    "results = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    questions = [q.strip() for q in f if q.strip()]\n",
    "\n",
    "print(f\"Loaded {len(questions)} questions.\")\n",
    "ft_count = 0\n",
    "\n",
    "for idx, question in enumerate(questions, start=1):\n",
    "    print(f\"\\n===== Processing Q{idx}: {question} =====\")\n",
    "\n",
    "    base_answer, ft_answer = compare_answer(question, max_new_tokens=512)\n",
    "\n",
    "    base_score = score_answer(question, base_answer)\n",
    "    ft_score = score_answer(question, ft_answer)\n",
    "\n",
    "    winner = \"ft\" if ft_score.get(\"overall\", 0) > base_score.get(\"overall\", 0) else \"base\"\n",
    "\n",
    "    if ft_score.get(\"overall\", 0) > base_score.get(\"overall\", 0):   ft_count += 1\n",
    "\n",
    "    entry = {\n",
    "        \"question\": question,\n",
    "        \"base_answer\": base_answer,\n",
    "        \"ft_answer\": ft_answer,\n",
    "        \"base_score\": base_score,\n",
    "        \"ft_score\": ft_score,\n",
    "        \"winner\": winner\n",
    "    }\n",
    "\n",
    "    results.append(entry)\n",
    "\n",
    "    with open(output_jsonl, \"a\", encoding=\"utf-8\") as fjson:\n",
    "        fjson.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 保存 CSV\n",
    "# with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "#     writer = csv.writer(fcsv)\n",
    "#     writer.writerow([\"Question\", \"Base Overall\", \"FT Overall\", \"Winner\"])\n",
    "\n",
    "#     for r in results:\n",
    "#         writer.writerow([\n",
    "#             r[\"question\"],\n",
    "#             r[\"base_score\"].get(\"overall\", 0),\n",
    "#             r[\"ft_score\"].get(\"overall\", 0),\n",
    "#             r[\"winner\"]\n",
    "#         ])\n",
    "\n",
    "print(\"\\n评估完成！\")\n",
    "print(\"JSONL:\", output_jsonl)\n",
    "print(f\"ft wins: {ft_count} times\")\n",
    "# print(\"CSV:\", output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
