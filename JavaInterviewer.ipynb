{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88cfc193",
   "metadata": {},
   "source": [
    "下载对应版本的transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb8628",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.41.2\n",
    "!pip install -q accelerate==0.30.1\n",
    "!pip install -q peft==0.11.1\n",
    "!pip install -q trl==0.8.6\n",
    "!pip install -q datasets==2.19.0\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q fsspec==2025.3.0\n",
    "!pip install -q gcsfs==2025.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af5973",
   "metadata": {},
   "source": [
    "由于要读取json文件的内容，因此需要在colab上将drive挂载到该python notebook中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d099e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a820a",
   "metadata": {},
   "source": [
    "查看一下GPU信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a608341",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21cceb",
   "metadata": {},
   "source": [
    "导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bba71",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/content/drive/MyDrive/java_sft/data/java_interview.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "# 默认会有一个 \"train\" split，这里再划分出验证集\n",
    "dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612afbbb",
   "metadata": {},
   "source": [
    "加载模型（Qwen/Qwen2.5-3B-Instruct）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5113f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"  # 比原来的 Base 版更适合做 Chat 微调\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 有些 Qwen 没显式 pad_token，这里兜底一下\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    load_in_4bit=True,     # 4bit 量化\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7a899",
   "metadata": {},
   "source": [
    "构建 LoRA 微调配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55684584",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    target_modules=[\"qkv_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef1281",
   "metadata": {},
   "source": [
    "构造 SFTTrainer（核心训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ef13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/java_sft/qwen-java-sft\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=200,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "# def format_example(example):\n",
    "#     prompt = (\n",
    "#         f\"<|im_start|>system\\n{example['system']}<|im_end|>\\n\"\n",
    "#         f\"<|im_start|>user\\n{example['input']}<|im_end|>\\n\"\n",
    "#         f\"<|im_start|>assistant\\n{example['output']}<|im_end|>\"\n",
    "#     )\n",
    "#     return [prompt]\n",
    "\n",
    "def format_example(batch):\n",
    "    results = []\n",
    "\n",
    "    # batch[\"system\"] 是一个 list，例如 [\"你是面试官\", \"你是面试官\", ...]\n",
    "    for sys_msg, usr_msg, asst_msg in zip(batch[\"system\"], batch[\"input\"], batch[\"output\"]):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_msg},\n",
    "            {\"role\": \"user\", \"content\": usr_msg},\n",
    "            {\"role\": \"assistant\", \"content\": asst_msg},\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        results.append(text)\n",
    "\n",
    "    return results    # <-- 必须是 list[str]\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=None,\n",
    "    max_seq_length=1024,\n",
    "    formatting_func=format_example,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210365c",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344a34b",
   "metadata": {},
   "source": [
    "保存 LoRA 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83754198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里保存的是 LoRA + 基座的 PeftModel\n",
    "trainer.model.save_pretrained(\"/content/drive/MyDrive/java_sft/qwen-java-sft\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/java_sft/qwen-java-sft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd70dfd",
   "metadata": {},
   "source": [
    "推理测试（加载 LoRA 模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944752bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# ---------------------------\n",
    "# 1. 加载微调前模型（Base Model）\n",
    "# ---------------------------\n",
    "base_model_name = model_name  # 和上面保持一致\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 加载微调后模型（LoRA + SFT）\n",
    "# ---------------------------\n",
    "ft_model_path = \"/content/drive/MyDrive/java_sft/qwen-java-sft\"\n",
    "\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    ft_model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if ft_tokenizer.pad_token is None:\n",
    "    ft_tokenizer.pad_token = ft_tokenizer.eos_token\n",
    "\n",
    "# 关键：使用 AutoPeftModelForCausalLM 加载带 LoRA 的模型\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    ft_model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=base_tokenizer)\n",
    "ft_pipe = pipeline(\"text-generation\", model=ft_model, tokenizer=ft_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a506939",
   "metadata": {},
   "source": [
    "定义答案生成与答案对比函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(pipe, tokenizer, question, max_new_tokens=256):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一名 Java 面试官。\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 先拿到 prompt 的长度\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(pipe.model.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,   # 面试场景更追求稳定，可以先关掉 sampling\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # 重新用 tokenizer 编码解码，截取「新生成的」部分\n",
    "    output_ids = tokenizer(outputs, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    new_tokens = output_ids[prompt_len:]  # 只取新生成 token\n",
    "\n",
    "    answer = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    return answer\n",
    "\n",
    "def compare_answer(question, max_new_tokens=256):\n",
    "    base_answer = generate_answer(base_pipe, base_tokenizer, question, max_new_tokens)\n",
    "    ft_answer = generate_answer(ft_pipe, ft_tokenizer, question, max_new_tokens)\n",
    "\n",
    "    # print(\"========【微调前】========\")\n",
    "    # print(base_answer)\n",
    "    # print(\"\\n========【微调后】========\")\n",
    "    # print(ft_answer)\n",
    "\n",
    "    return base_answer, ft_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dfbb03",
   "metadata": {},
   "source": [
    "验证及对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ef654",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Redis的持久化策略有两种，分别是什么？以及两者的区别是什么？\"\n",
    "res = compare_answer(question, 1024)\n",
    "print(\"========【微调前】========\")\n",
    "print(res[0])\n",
    "print(\"\\n========【微调后】========\")\n",
    "print(res[1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
